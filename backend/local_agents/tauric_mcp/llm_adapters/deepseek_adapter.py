"""
DeepSeek LLMé€‚é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨ç»Ÿè®¡
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.outputs import ChatResult
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from local_agents.tauric_mcp.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    print("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")


class ChatDeepSeek(ChatOpenAI):
    """
    DeepSeekèŠå¤©æ¨¡å‹é€‚é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨ç»Ÿè®¡
    
    ç»§æ‰¿è‡ªChatOpenAIï¼Œæ·»åŠ äº†Tokenä½¿ç”¨é‡ç»Ÿè®¡åŠŸèƒ½
    """
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        base_url: str = "https://api.deepseek.com",
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–DeepSeeké€‚é…å™¨
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºdeepseek-chat
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡DEEPSEEK_API_KEYè·å–
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # è·å–APIå¯†é’¥
        if api_key is None:
            api_key = os.getenv("DEEPSEEK_API_KEY")
            if not api_key:
                raise ValueError("DeepSeek APIå¯†é’¥æœªæ‰¾åˆ°ã€‚è¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°ã€‚")
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            model=model,
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        self.model_name = model
        
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œå¹¶è®°å½•tokenä½¿ç”¨é‡
        """

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æå–å¹¶ç§»é™¤è‡ªå®šä¹‰å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™çˆ¶ç±»
        session_id = kwargs.pop('session_id', None)
        analysis_type = kwargs.pop('analysis_type', None)

        try:
            # Debug: Print message types
            print(f"ğŸ” [DeepSeek] Input type: {type(messages)}")

            # Handle ChatPromptValue which contains messages
            if hasattr(messages, 'messages'):
                actual_messages = messages.messages
                print(f"ğŸ” [DeepSeek] Extracted messages from ChatPromptValue")
            else:
                actual_messages = messages

            if hasattr(actual_messages, '__len__'):
                print(f"ğŸ” [DeepSeek] Processing {len(actual_messages)} messages")
                for i, msg in enumerate(actual_messages):
                    print(f"ğŸ” [DeepSeek] Message {i}: type={type(msg)}, content={type(msg.content) if hasattr(msg, 'content') else 'N/A'}")
            else:
                print(f"ğŸ” [DeepSeek] Messages object: {actual_messages}")

            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥ç¡®ä¿å…¼å®¹æ€§
            converted_messages = []
            for msg in actual_messages:
                # Handle tuple messages
                if isinstance(msg, tuple):
                    role, content = msg
                    if role == "human":
                        converted_messages.append(HumanMessage(content=content))
                    elif role == "system":
                        converted_messages.append(SystemMessage(content=content))
                    elif role == "ai" or role == "assistant":
                        converted_messages.append(AIMessage(content=content))
                    else:
                        converted_messages.append(HumanMessage(content=str(content)))
                elif isinstance(msg, BaseMessage):
                    if isinstance(msg.content, list):
                        # å¦‚æœå†…å®¹æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæå–æ–‡æœ¬å†…å®¹
                        text_content = ""
                        for item in msg.content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                text_content += item.get("text", "")
                        # åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡
                        if isinstance(msg, HumanMessage):
                            converted_messages.append(HumanMessage(content=text_content))
                        elif isinstance(msg, AIMessage):
                            converted_messages.append(AIMessage(content=text_content))
                        elif isinstance(msg, SystemMessage):
                            converted_messages.append(SystemMessage(content=text_content))
                        elif isinstance(msg, ToolMessage):
                            # ToolMessage éœ€è¦ä¿ç•™ tool_call_id
                            converted_messages.append(ToolMessage(
                                content=text_content,
                                tool_call_id=msg.tool_call_id
                            ))
                        else:
                            converted_messages.append(HumanMessage(content=text_content))
                    else:
                        converted_messages.append(msg)
                else:
                    # Handle unexpected message types
                    print(f"âš ï¸ [DeepSeek] Unexpected message type: {type(msg)}")
                    converted_messages.append(HumanMessage(content=str(msg)))

            # ä½¿ç”¨è½¬æ¢åçš„æ¶ˆæ¯è°ƒç”¨çˆ¶ç±»æ–¹æ³•
            result = super()._generate(converted_messages, stop, run_manager, **kwargs)
            
            # æå–tokenä½¿ç”¨é‡
            input_tokens = 0
            output_tokens = 0
            
            # å°è¯•ä»å“åº”ä¸­æå–tokenä½¿ç”¨é‡
            if hasattr(result, 'llm_output') and result.llm_output:
                token_usage = result.llm_output.get('token_usage', {})
                if token_usage:
                    input_tokens = token_usage.get('prompt_tokens', 0)
                    output_tokens = token_usage.get('completion_tokens', 0)
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°tokenä½¿ç”¨é‡ï¼Œè¿›è¡Œä¼°ç®—
            if input_tokens == 0 and output_tokens == 0:
                input_tokens = self._estimate_input_tokens(messages)
                output_tokens = self._estimate_output_tokens(result)
                print(f"ğŸ” [DeepSeek] ä½¿ç”¨ä¼°ç®—token: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            else:
                print(f"ğŸ“Š [DeepSeek] å®é™…tokenä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            
            # è®°å½•tokenä½¿ç”¨é‡
            if TOKEN_TRACKING_ENABLED and (input_tokens > 0 or output_tokens > 0):
                try:
                    # ä½¿ç”¨æå–çš„å‚æ•°æˆ–ç”Ÿæˆé»˜è®¤å€¼
                    if session_id is None:
                        session_id = f"deepseek_{hash(str(messages))%10000}"
                    if analysis_type is None:
                        analysis_type = 'stock_analysis'

                    # è®°å½•ä½¿ç”¨é‡
                    usage_record = token_tracker.track_usage(
                        provider="deepseek",
                        model_name=self.model_name,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=session_id,
                        analysis_type=analysis_type
                    )

                    if usage_record:
                        if usage_record.cost == 0.0:
                            print(f"âš ï¸ [DeepSeek] æˆæœ¬è®¡ç®—ä¸º0ï¼Œå¯èƒ½é…ç½®æœ‰é—®é¢˜")
                        else:
                            print(f"ğŸ’° [DeepSeek] æœ¬æ¬¡è°ƒç”¨æˆæœ¬: Â¥{usage_record.cost:.6f}")
                        print(f"ğŸ“Š [DeepSeek] å®é™…tokenä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
                    else:
                        print(f"âš ï¸ [DeepSeek] æœªåˆ›å»ºä½¿ç”¨è®°å½•")

                except Exception as track_error:
                    print(f"âš ï¸ [DeepSeek] Tokenç»Ÿè®¡å¤±è´¥: {track_error}")
                    import traceback
                    traceback.print_exc()
            
            return result
            
        except Exception as e:
            print(f"âŒ [DeepSeek] è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _convert_messages_to_deepseek_format(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """
        å°†LangChainæ¶ˆæ¯è½¬æ¢ä¸ºDeepSeek APIæ ¼å¼

        Args:
            messages: LangChainæ¶ˆæ¯åˆ—è¡¨

        Returns:
            DeepSeek APIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        deepseek_messages = []

        for message in messages:
            # ç¡®å®šè§’è‰²
            if isinstance(message, SystemMessage):
                role = "system"
            elif isinstance(message, AIMessage):
                role = "assistant"
            elif isinstance(message, HumanMessage):
                role = "user"
            else:
                # é»˜è®¤ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
                role = "user"

            # å¤„ç†æ¶ˆæ¯å†…å®¹
            content = message.content
            if isinstance(content, list):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
                text_content = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_content += item.get("text", "")
                content = text_content

            # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
            deepseek_messages.append({
                "role": role,
                "content": str(content)
            })

        return deepseek_messages

    def _estimate_input_tokens(self, messages: List[BaseMessage]) -> int:
        """
        ä¼°ç®—è¾“å…¥tokenæ•°é‡
        
        Args:
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            ä¼°ç®—çš„è¾“å…¥tokenæ•°é‡
        """
        total_chars = 0
        for message in messages:
            if hasattr(message, 'content'):
                total_chars += len(str(message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def _estimate_output_tokens(self, result: ChatResult) -> int:
        """
        ä¼°ç®—è¾“å‡ºtokenæ•°é‡
        
        Args:
            result: èŠå¤©ç»“æœ
            
        Returns:
            ä¼°ç®—çš„è¾“å‡ºtokenæ•°é‡
        """
        total_chars = 0
        for generation in result.generations:
            if hasattr(generation, 'message') and hasattr(generation.message, 'content'):
                total_chars += len(str(generation.message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def invoke(
        self,
        input: Union[str, List[BaseMessage]],
        config: Optional[Dict] = None,
        **kwargs: Any,
    ) -> AIMessage:
        """
        è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”
        
        Args:
            input: è¾“å…¥æ¶ˆæ¯
            config: é…ç½®å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆåŒ…æ‹¬session_idå’Œanalysis_typeï¼‰
            
        Returns:
            AIæ¶ˆæ¯å“åº”
        """
        
        # å¤„ç†è¾“å…¥
        if isinstance(input, str):
            messages = [HumanMessage(content=input)]
        else:
            messages = input
        
        # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
        result = self._generate(messages, **kwargs)
        
        # è¿”å›ç¬¬ä¸€ä¸ªç”Ÿæˆç»“æœçš„æ¶ˆæ¯
        if result.generations:
            return result.generations[0].message
        else:
            return AIMessage(content="")


def create_deepseek_llm(
    model: str = "deepseek-chat",
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    **kwargs
) -> ChatDeepSeek:
    """
    åˆ›å»ºDeepSeek LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ChatDeepSeekå®ä¾‹
    """
    return ChatDeepSeek(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åˆ«å
DeepSeekLLM = ChatDeepSeek
